# services/hf_client.py
import streamlit as st
import requests

HF_TOKEN = st.secrets["HUGGINGFACE_API_TOKEN"]
HEADERS = {
    "Authorization": f"Bearer {HF_TOKEN}",
    "Content-Type": "application/json"
}

# ‚úÖ NUEVO MODELO - SmolLM3-3B
MODEL = "HuggingFaceTB/SmolLM3-3B"

def hf_generate(prompt: str, enable_thinking: bool = False) -> str:
    # URL correcta para la API de inferencia
    url = f"https://router.huggingface.co/models/{MODEL}"
    
    # üîß Aprovecha las caracter√≠sticas del modelo: estructura el prompt para chat
    messages = [
        {"role": "user", "content": prompt}
    ]
    
    # Construye el prompt con el formato que espera el modelo
    chat_prompt = f"<|user|>\n{prompt}\n<|assistant|>\n"
    
    payload = {
        "inputs": chat_prompt,
        "parameters": {
            "max_new_tokens": 1500,  # Aumentado para respuestas m√°s completas
            "temperature": 0.6,      # Recomendado por los autores del modelo
            "top_p": 0.95,           # Recomendado por los autores
            "return_full_text": False,
            "do_sample": True
        }
    }
    
    # Si quieres desactivar el modo "thinking" (razonamiento extendido)
    if not enable_thinking:
        # Puedes a√±adir una instrucci√≥n al sistema
        system_instruction = "/no_think\n"
        chat_prompt = f"<|system|>\n{system_instruction}<|user|>\n{prompt}\n<|assistant|>\n"
        payload["inputs"] = chat_prompt

    try:
        response = requests.post(
            url,
            headers=HEADERS,
            json=payload,
            timeout=45  # Un poco m√°s de tiempo para modelos m√°s grandes
        )
        
        if response.status_code == 503:
            # Modelo carg√°ndose - com√∫n con modelos reci√©n usados
            return "üîÑ El modelo se est√° cargando. Por favor, intenta de nuevo en 20-30 segundos."
        elif response.status_code != 200:
            return f"‚ùå Error ({response.status_code}): {response.text[:200]}"
        
        data = response.json()
        
        # Extrae la respuesta del formato de la API
        if isinstance(data, list) and len(data) > 0:
            if "generated_text" in data[0]:
                return data[0]["generated_text"].strip()
        
        # Formato alternativo
        if isinstance(data, dict) and "generated_text" in data:
            return data["generated_text"].strip()
            
        return str(data)[:2000]  # Limita la respuesta si viene en formato inesperado
        
    except requests.exceptions.Timeout:
        return "‚è±Ô∏è La solicitud tard√≥ demasiado. El modelo podr√≠a estar muy ocupado."
    except Exception as e:
        return f"‚ùå Error de conexi√≥n: {str(e)}"
